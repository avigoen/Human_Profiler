{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, json, argparse\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn=frame.copy()\n",
    "    frameHeight=frameOpencvDnn.shape[0]\n",
    "    frameWidth=frameOpencvDnn.shape[1]\n",
    "    blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections=net.forward()\n",
    "    faceBoxes=[]\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence=detections[0,0,i,2]\n",
    "        if confidence>conf_threshold:\n",
    "            x1=int(detections[0,0,i,3]*frameWidth)\n",
    "            y1=int(detections[0,0,i,4]*frameHeight)\n",
    "            x2=int(detections[0,0,i,5]*frameWidth)\n",
    "            y2=int(detections[0,0,i,6]*frameHeight)\n",
    "            faceBoxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn,faceBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--image')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male','Female']\n",
    "\n",
    "# Directory paths to models\n",
    "model_dir_path = os.path.join(os.getcwd(), \"models\")\n",
    "gender_model_dirpath = os.path.join(model_dir_path, \"gender\")\n",
    "age_model_dirpath = os.path.join(model_dir_path, \"age\")\n",
    "face_model_dirpath = os.path.join(model_dir_path, \"opencv_face\")\n",
    "celeb_model_dirpath = os.path.join(model_dir_path, \"celeb_similarity\")\n",
    "emotion_model_dirpath = os.path.join(model_dir_path, \"emotion\")\n",
    "\n",
    "# File path to models\n",
    "face_model_path = os.path.join(face_model_dirpath, \"opencv_face_detector_uint8.pb\")\n",
    "face_model_text_path = os.path.join(face_model_dirpath, \"opencv_face_detector.pbtxt\")\n",
    "\n",
    "age_model_path = os.path.join(age_model_dirpath, \"age_net.caffemodel\")\n",
    "age_model_text_path = os.path.join(age_model_dirpath, \"age_deploy.prototxt\")\n",
    "\n",
    "gender_model_path = os.path.join(gender_model_dirpath, \"gender_net.caffemodel\")\n",
    "gender_model_text_path = os.path.join(gender_model_dirpath, \"gender_deploy.prototxt\")\n",
    "\n",
    "celeb_model_path = os.path.join(celeb_model_dirpath, \"mobilenet.h5\")\n",
    "celeb_model_classes_path = os.path.join(celeb_model_dirpath, \"celeb_classes.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceNet=cv2.dnn.readNet(face_model_path, face_model_text_path)\n",
    "ageNet=cv2.dnn.readNet(age_model_path, age_model_text_path)\n",
    "genderNet=cv2.dnn.readNet(gender_model_path, gender_model_text_path)\n",
    "\n",
    "tf_emotion_model = tf.keras.models.load_model(emotion_model_path) \n",
    "tf_celeb_model = tf.keras.models.load_model(celeb_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebs_option = []\n",
    "with open(celeb_model_classes_path, 'r') as infile:\n",
    "    celebs_option = json.loads(json.load(infile))\n",
    "    celebs_option = list(map(lambda x: x.split(\"_\")[-1], celebs_option))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_celebrity(face):\n",
    "    resized_img = cv2.resize(face, (160,160), interpolation = cv2.INTER_AREA)\n",
    "    resized_img = np.reshape(resized_img,(-1,160,160,3))/255.0\n",
    "    result = np.argmax(tf_celeb_model.predict(resized_img), axis=-1)\n",
    "    return celebs_option[result[-1]]\n",
    "\n",
    "def predict_gender(faceBlob):\n",
    "    genderNet.setInput(faceBlob)\n",
    "    genderPreds=genderNet.forward()\n",
    "    gender=genderList[genderPreds[0].argmax()]\n",
    "    return gender\n",
    "\n",
    "def predict_age(faceBlob):\n",
    "    ageNet.setInput(faceBlob)\n",
    "    agePreds=ageNet.forward()\n",
    "    age=ageList[agePreds[0].argmax()]\n",
    "    return age\n",
    "    \n",
    "def moodNamePrintFromLabel(n):\n",
    "    if n == 0: result = 'Angry '\n",
    "    elif n == 1: result = 'Disgust '\n",
    "    elif n == 2: result = 'Fear'\n",
    "    elif n == 3: result = 'Happy'\n",
    "    elif n == 4: result = 'Sad'\n",
    "    elif n == 5: result = 'Surprise'\n",
    "    elif n == 6: result = 'Neutral'\n",
    "    return result\n",
    "\n",
    "def predict_emotion(face):\n",
    "    resized_img = cv2.resize(face, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    resized_img = np.reshape(resized_img,(-1,224,224,3))/255.0\n",
    "    result = np.argmax(tf_emotion_model.predict(resized_img), axis=-1)\n",
    "    return moodNamePrintFromLabel(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 623ms/step\n",
      "1/1 [==============================] - 1s 771ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "No face detected\n",
      "No face detected\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "No face detected\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(args.image if args.image else 0)\n",
    "padding = 20\n",
    "\n",
    "continue_program = True\n",
    "\n",
    "while continue_program:\n",
    "    hasFrame, frame = video.read()\n",
    "    if not hasFrame:\n",
    "        continue\n",
    "    continue_program = False\n",
    "    resultImg, faceBoxes = highlightFace(faceNet, frame)\n",
    "    if not faceBoxes:\n",
    "        continue_program = True\n",
    "        print(\"No face detected\")\n",
    "\n",
    "    for faceBox in faceBoxes:\n",
    "        face = frame[max(0, faceBox[1]-padding):\n",
    "                     min(faceBox[3]+padding, frame.shape[0]-1), max(0, faceBox[0]-padding):min(faceBox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "\n",
    "        gender = predict_gender(blob)\n",
    "        age = predict_age(blob)\n",
    "        predicted_actor = predict_celebrity(face)\n",
    "        emotion = predict_emotion(face)\n",
    "\n",
    "        text = f'{gender}, {age}, {predicted_actor}, {emotion}'\n",
    "        x0 = faceBox[0]\n",
    "        y0 = faceBox[1]\n",
    "        cv2.putText(resultImg, text, (x0, y0),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"window\", resultImg)\n",
    "        key = cv2.waitKey(10)\n",
    "        continue_program = True\n",
    "        if key == 27:\n",
    "            continue_program = False\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
